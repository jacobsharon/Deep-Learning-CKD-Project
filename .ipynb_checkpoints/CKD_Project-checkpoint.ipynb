{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1080c16-8444-42f0-847b-8733f0729a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================\n",
    "# Global Imports \n",
    "# ================\n",
    "\n",
    "# Dataset\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Data / Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a937d508-c521-416a-b6ad-cd582add1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
      "0    48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "1     7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "2    62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
      "3    48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
      "4    51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
      "..    ...   ...    ...  ...  ...     ...       ...         ...         ...   \n",
      "395  55.0  80.0  1.020  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "396  42.0  70.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "397  12.0  80.0  1.020  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "398  17.0  60.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "399  58.0  80.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "\n",
      "       bgr  ...  hemo   pcv    wbcc  rbcc  htn   dm  cad  appet   pe  ane  \n",
      "0    121.0  ...  15.4  44.0  7800.0   5.2  yes  yes   no   good   no   no  \n",
      "1      NaN  ...  11.3  38.0  6000.0   NaN   no   no   no   good   no   no  \n",
      "2    423.0  ...   9.6  31.0  7500.0   NaN   no  yes   no   poor   no  yes  \n",
      "3    117.0  ...  11.2  32.0  6700.0   3.9  yes   no   no   poor  yes  yes  \n",
      "4    106.0  ...  11.6  35.0  7300.0   4.6   no   no   no   good   no   no  \n",
      "..     ...  ...   ...   ...     ...   ...  ...  ...  ...    ...  ...  ...  \n",
      "395  140.0  ...  15.7  47.0  6700.0   4.9   no   no   no   good   no   no  \n",
      "396   75.0  ...  16.5  54.0  7800.0   6.2   no   no   no   good   no   no  \n",
      "397  100.0  ...  15.8  49.0  6600.0   5.4   no   no   no   good   no   no  \n",
      "398  114.0  ...  14.2  51.0  7200.0   5.9   no   no   no   good   no   no  \n",
      "399  131.0  ...  15.8  53.0  6800.0   6.1   no   no   no   good   no   no  \n",
      "\n",
      "[400 rows x 24 columns]\n",
      "0         ckd\n",
      "1         ckd\n",
      "2         ckd\n",
      "3         ckd\n",
      "4         ckd\n",
      "        ...  \n",
      "395    notckd\n",
      "396    notckd\n",
      "397    notckd\n",
      "398    notckd\n",
      "399    notckd\n",
      "Name: class, Length: 400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Load UCI CKD Dataset\n",
    "# ==========================\n",
    "\n",
    "chronic_kidney_disease = fetch_ucirepo(id=336)\n",
    "\n",
    "# Extract features and target\n",
    "X = chronic_kidney_disease.data.features\n",
    "y = chronic_kidney_disease.data.targets[\"class\"]\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6faef85-91cf-4b33-81d8-d5d0ae26a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: Index(['age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr', 'bu',\n",
      "       'sc', 'sod', 'pot', 'hemo', 'pcv', 'wbcc', 'rbcc', 'htn', 'dm', 'cad',\n",
      "       'appet', 'pe', 'ane'],\n",
      "      dtype='object')\n",
      "Categorical cols: Index([], dtype='object')\n",
      "age      float32\n",
      "bp       float32\n",
      "sg       float32\n",
      "al       float32\n",
      "su       float32\n",
      "rbc      float32\n",
      "pc       float32\n",
      "pcc      float32\n",
      "ba       float32\n",
      "bgr      float32\n",
      "bu       float32\n",
      "sc       float32\n",
      "sod      float32\n",
      "pot      float32\n",
      "hemo     float32\n",
      "pcv      float32\n",
      "wbcc     float32\n",
      "rbcc     float32\n",
      "htn      float32\n",
      "dm       float32\n",
      "cad      float32\n",
      "appet    float32\n",
      "pe       float32\n",
      "ane      float32\n",
      "dtype: object\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Data Preprocessing\n",
    "# ==========================\n",
    "\n",
    "# 1. Normalize string formatting\n",
    "X = X.apply(lambda col: col.map(\n",
    "    lambda v: v.strip().lower() if isinstance(v, str) else v\n",
    "))\n",
    "\n",
    "# 2. Binary mapping for categorical yes/no–type fields\n",
    "binary_map = {\n",
    "    'yes': 1, 'no': 0,\n",
    "    'present': 1, 'notpresent': 0,\n",
    "    'abnormal': 1, 'normal': 0,\n",
    "    'good': 1, 'poor': 0\n",
    "}\n",
    "\n",
    "binary_cols = ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']\n",
    "\n",
    "for col in binary_cols:\n",
    "    X[col] = X[col].map(binary_map)\n",
    "\n",
    "# 3. Ensure binary columns are numeric\n",
    "X[binary_cols] = X[binary_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 4. Identify numerical/categorical columns\n",
    "num_cols = X.select_dtypes(include=['number']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "\n",
    "# 5. KNN imputation for numerical columns\n",
    "imputer = KNNImputer()\n",
    "X.loc[:, num_cols] = imputer.fit_transform(X[num_cols])\n",
    "\n",
    "# 6. Min–Max normalization\n",
    "scaler = MinMaxScaler()\n",
    "X.loc[:, num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# ---- Encode target labels ----\n",
    "y = y.apply(lambda v: v.strip().lower() if isinstance(v, str) else v)\n",
    "\n",
    "if isinstance(y, pd.DataFrame):\n",
    "    y = y.squeeze()\n",
    "\n",
    "y = y.map({'ckd': 1, 'notckd': 0})\n",
    "\n",
    "# Convert to float32\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "\n",
    "print(X.dtypes)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f109818d-efb0-4b2e-8850-f23079eb62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Train/Test Split + Tensors\n",
    "# ==========================\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values,\n",
    "    y.values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor  = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Dataset + Loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor,  y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5212286-3a17-4626-9912-7949ce3d2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# MLP Model Definition\n",
    "# ==========================\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected feedforward neural network (Multi-Layer Perceptron)\n",
    "    used for binary classification of Chronic Kidney Disease (CKD).\n",
    "\n",
    "    This implementation supports an arbitrary number of hidden layers, each\n",
    "    followed by:\n",
    "      • Batch Normalization (except after the first layer)\n",
    "      • Dropout (p = 0.02)\n",
    "      • ReLU activation\n",
    "\n",
    "    Kaiming uniform initialization is applied to all Linear layer weights,\n",
    "    and biases are initialized to zero. The final layer outputs an unbounded\n",
    "    logit suitable for use with BCEWithLogitsLoss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        Number of input features. For this CKD dataset, this is 24.\n",
    "\n",
    "    hidden_sizes : list of int\n",
    "        A list specifying the width of each hidden layer.\n",
    "        Example: [64, 32] produces a network with two hidden layers.\n",
    "\n",
    "    output_size : int\n",
    "        Number of output neurons. For binary classification, this is 1.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    network : nn.Sequential\n",
    "        The sequential model containing all layers in forward order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # First hidden layer (no batch norm / dropout here)\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Remaining hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_sizes[i + 1]))\n",
    "            layers.append(nn.Dropout(p=0.02))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer (produces logits)\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        # Construct sequential model\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize parameters\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights of all Linear layers using Kaiming uniform\n",
    "        initialization, which is recommended for ReLU-activated networks.\n",
    "\n",
    "        Bias terms are initialized to zero.\n",
    "        \"\"\"\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "                if layer.bias is not None:\n",
    "                    init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, input_size).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output logits of shape (batch_size, output_size).\n",
    "            These logits should be passed to sigmoid() or directly to\n",
    "            BCEWithLogitsLoss for stable binary classification.\n",
    "        \"\"\"\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a2ac3f-d5f8-462a-a8f4-14c676ca16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Metrics Utilities\n",
    "# ==================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model whose trainable parameters will be counted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Total number of trainable parameters.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def threshold_sweep(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Perform a threshold sweep from 0 to 1 to find the threshold that\n",
    "    maximizes F1-score. Also compute accuracy, precision, recall,\n",
    "    specificity, and Youden's J statistic at the best threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of int\n",
    "        Ground-truth binary labels for the validation set.\n",
    "\n",
    "    y_prob : array-like of float\n",
    "        Predicted sigmoid probabilities for the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_thr : float\n",
    "        Threshold producing the highest F1-score.\n",
    "\n",
    "    best_metrics : dict\n",
    "        Dictionary containing accuracy, precision, recall, specificity,\n",
    "        and F1-score at the best threshold.\n",
    "\n",
    "    best_J : float\n",
    "        Youden’s J statistic at the best threshold.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    eps = 1e-8\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_thr = 0.5\n",
    "    best_metrics = None\n",
    "    best_J = None\n",
    "\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "        precision = tp / (tp + fp + eps)\n",
    "        recall = tp / (tp + fn + eps)\n",
    "        specificity = tn / (tn + fp + eps)\n",
    "        f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "        J = recall + specificity - 1\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "            best_metrics = {\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"specificity\": specificity,\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "            best_J = J\n",
    "\n",
    "    return best_thr, best_metrics, best_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a499967b-ad31-4c72-a191-8719efe6f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================\n",
      "Training Model A_64_32\n",
      "==============================\n",
      "\n",
      "---- Fold 1 ----\n",
      "Epoch 20/200 | Train 0.0531 | Val 0.0447\n",
      "Epoch 40/200 | Train 0.0256 | Val 0.0191\n",
      "Epoch 60/200 | Train 0.0179 | Val 0.0143\n",
      "Epoch 80/200 | Train 0.0075 | Val 0.0110\n",
      "Early stopping triggered at epoch 86\n",
      "\n",
      "---- Fold 2 ----\n",
      "Epoch 20/200 | Train 0.0540 | Val 0.0617\n",
      "Epoch 40/200 | Train 0.0177 | Val 0.0454\n",
      "Epoch 60/200 | Train 0.0102 | Val 0.0474\n",
      "Early stopping triggered at epoch 69\n",
      "\n",
      "---- Fold 3 ----\n",
      "Epoch 20/200 | Train 0.0662 | Val 0.0709\n",
      "Epoch 40/200 | Train 0.0273 | Val 0.0497\n",
      "Epoch 60/200 | Train 0.0112 | Val 0.0473\n",
      "Early stopping triggered at epoch 76\n",
      "\n",
      "---- Fold 4 ----\n",
      "Epoch 20/200 | Train 0.0489 | Val 0.0837\n",
      "Epoch 40/200 | Train 0.0264 | Val 0.0463\n",
      "Epoch 60/200 | Train 0.0106 | Val 0.0321\n",
      "Epoch 80/200 | Train 0.0051 | Val 0.0238\n",
      "Epoch 100/200 | Train 0.0046 | Val 0.0190\n",
      "Early stopping triggered at epoch 120\n",
      "\n",
      "---- Fold 5 ----\n",
      "Epoch 20/200 | Train 0.0700 | Val 0.0942\n",
      "Epoch 40/200 | Train 0.0311 | Val 0.0418\n",
      "Epoch 60/200 | Train 0.0174 | Val 0.0228\n",
      "Epoch 80/200 | Train 0.0144 | Val 0.0132\n",
      "Epoch 100/200 | Train 0.0077 | Val 0.0089\n",
      "Epoch 120/200 | Train 0.0117 | Val 0.0063\n",
      "Epoch 140/200 | Train 0.0024 | Val 0.0047\n",
      "Epoch 160/200 | Train 0.0032 | Val 0.0028\n",
      "Early stopping triggered at epoch 168\n",
      "\n",
      ">>> Mean F1 for A_64_32: 1.0000\n",
      "\n",
      "\n",
      "==============================\n",
      "Training Model B_256\n",
      "==============================\n",
      "\n",
      "---- Fold 1 ----\n",
      "Epoch 20/200 | Train 0.0815 | Val 0.0558\n",
      "Epoch 40/200 | Train 0.0411 | Val 0.0300\n",
      "Epoch 60/200 | Train 0.0257 | Val 0.0242\n",
      "Epoch 80/200 | Train 0.0182 | Val 0.0229\n",
      "Early stopping triggered at epoch 94\n",
      "\n",
      "---- Fold 2 ----\n",
      "Epoch 20/200 | Train 0.0707 | Val 0.0575\n",
      "Epoch 40/200 | Train 0.0362 | Val 0.0423\n",
      "Epoch 60/200 | Train 0.0225 | Val 0.0431\n",
      "Early stopping triggered at epoch 64\n",
      "\n",
      "---- Fold 3 ----\n",
      "Epoch 20/200 | Train 0.0739 | Val 0.1057\n",
      "Epoch 40/200 | Train 0.0383 | Val 0.0788\n",
      "Epoch 60/200 | Train 0.0245 | Val 0.0716\n",
      "Early stopping triggered at epoch 75\n",
      "\n",
      "---- Fold 4 ----\n",
      "Epoch 20/200 | Train 0.0570 | Val 0.0959\n",
      "Epoch 40/200 | Train 0.0284 | Val 0.0745\n",
      "Epoch 60/200 | Train 0.0187 | Val 0.0636\n",
      "Epoch 80/200 | Train 0.0133 | Val 0.0580\n",
      "Epoch 100/200 | Train 0.0100 | Val 0.0526\n",
      "Epoch 120/200 | Train 0.0076 | Val 0.0494\n",
      "Epoch 140/200 | Train 0.0059 | Val 0.0467\n",
      "Epoch 160/200 | Train 0.0047 | Val 0.0433\n",
      "Epoch 180/200 | Train 0.0038 | Val 0.0412\n",
      "Epoch 200/200 | Train 0.0030 | Val 0.0390\n",
      "\n",
      "---- Fold 5 ----\n",
      "Epoch 20/200 | Train 0.0742 | Val 0.0998\n",
      "Epoch 40/200 | Train 0.0445 | Val 0.0597\n",
      "Epoch 60/200 | Train 0.0309 | Val 0.0395\n",
      "Epoch 80/200 | Train 0.0229 | Val 0.0268\n",
      "Epoch 100/200 | Train 0.0183 | Val 0.0194\n",
      "Epoch 120/200 | Train 0.0144 | Val 0.0149\n",
      "Epoch 140/200 | Train 0.0115 | Val 0.0117\n",
      "Epoch 160/200 | Train 0.0094 | Val 0.0095\n",
      "Epoch 180/200 | Train 0.0078 | Val 0.0080\n",
      "Epoch 200/200 | Train 0.0068 | Val 0.0067\n",
      "\n",
      ">>> Mean F1 for B_256: 0.9941\n",
      "\n",
      "\n",
      "==============================\n",
      "Training Model C_64_32_16_8\n",
      "==============================\n",
      "\n",
      "---- Fold 1 ----\n",
      "Epoch 20/200 | Train 0.1026 | Val 0.1049\n",
      "Epoch 40/200 | Train 0.0428 | Val 0.0413\n",
      "Epoch 60/200 | Train 0.0240 | Val 0.0318\n",
      "Early stopping triggered at epoch 73\n",
      "\n",
      "---- Fold 2 ----\n",
      "Epoch 20/200 | Train 0.1001 | Val 0.1278\n",
      "Epoch 40/200 | Train 0.0416 | Val 0.0798\n",
      "Epoch 60/200 | Train 0.0214 | Val 0.0968\n",
      "Early stopping triggered at epoch 63\n",
      "\n",
      "---- Fold 3 ----\n",
      "Epoch 20/200 | Train 0.1098 | Val 0.1011\n",
      "Epoch 40/200 | Train 0.0502 | Val 0.0568\n",
      "Epoch 60/200 | Train 0.0237 | Val 0.0541\n",
      "Epoch 80/200 | Train 0.0137 | Val 0.0494\n",
      "Early stopping triggered at epoch 98\n",
      "\n",
      "---- Fold 4 ----\n",
      "Epoch 20/200 | Train 0.1215 | Val 0.1366\n",
      "Epoch 40/200 | Train 0.0588 | Val 0.0771\n",
      "Epoch 60/200 | Train 0.0231 | Val 0.0531\n",
      "Early stopping triggered at epoch 67\n",
      "\n",
      "---- Fold 5 ----\n",
      "Epoch 20/200 | Train 0.1043 | Val 0.1157\n",
      "Epoch 40/200 | Train 0.0563 | Val 0.0567\n",
      "Epoch 60/200 | Train 0.0259 | Val 0.0219\n",
      "Epoch 80/200 | Train 0.0142 | Val 0.0123\n",
      "Epoch 100/200 | Train 0.0080 | Val 0.0091\n",
      "Epoch 120/200 | Train 0.0120 | Val 0.0058\n",
      "Epoch 140/200 | Train 0.0169 | Val 0.0043\n",
      "Early stopping triggered at epoch 157\n",
      "\n",
      ">>> Mean F1 for C_64_32_16_8: 0.9980\n",
      "\n",
      "\n",
      "==============================\n",
      "Training Model D_128_32_128\n",
      "==============================\n",
      "\n",
      "---- Fold 1 ----\n",
      "Epoch 20/200 | Train 0.0093 | Val 0.0192\n",
      "Epoch 40/200 | Train 0.0031 | Val 0.0243\n",
      "Early stopping triggered at epoch 60\n",
      "\n",
      "---- Fold 2 ----\n",
      "Epoch 20/200 | Train 0.0186 | Val 0.0289\n",
      "Epoch 40/200 | Train 0.0152 | Val 0.0371\n",
      "Early stopping triggered at epoch 42\n",
      "\n",
      "---- Fold 3 ----\n",
      "Epoch 20/200 | Train 0.0270 | Val 0.0454\n",
      "Early stopping triggered at epoch 40\n",
      "\n",
      "---- Fold 4 ----\n",
      "Epoch 20/200 | Train 0.0120 | Val 0.0417\n",
      "Epoch 40/200 | Train 0.0112 | Val 0.0188\n",
      "Epoch 60/200 | Train 0.0036 | Val 0.0124\n",
      "Early stopping triggered at epoch 70\n",
      "\n",
      "---- Fold 5 ----\n",
      "Epoch 20/200 | Train 0.0293 | Val 0.0209\n",
      "Epoch 40/200 | Train 0.0077 | Val 0.0058\n",
      "Epoch 60/200 | Train 0.0031 | Val 0.0040\n",
      "Epoch 80/200 | Train 0.0093 | Val 0.0150\n",
      "Epoch 100/200 | Train 0.0020 | Val 0.0022\n",
      "Epoch 120/200 | Train 0.0030 | Val 0.0017\n",
      "Epoch 140/200 | Train 0.0009 | Val 0.0035\n",
      "Early stopping triggered at epoch 150\n",
      "\n",
      ">>> Mean F1 for D_128_32_128: 0.9960\n",
      "\n",
      "\n",
      "==============================\n",
      "Training Model E_64_32_16\n",
      "==============================\n",
      "\n",
      "---- Fold 1 ----\n",
      "Epoch 20/200 | Train 0.0773 | Val 0.0671\n",
      "Epoch 40/200 | Train 0.0307 | Val 0.0464\n",
      "Early stopping triggered at epoch 56\n",
      "\n",
      "---- Fold 2 ----\n",
      "Epoch 20/200 | Train 0.0719 | Val 0.0827\n",
      "Epoch 40/200 | Train 0.0345 | Val 0.0564\n",
      "Epoch 60/200 | Train 0.0150 | Val 0.0574\n",
      "Epoch 80/200 | Train 0.0107 | Val 0.0315\n",
      "Early stopping triggered at epoch 88\n",
      "\n",
      "---- Fold 3 ----\n",
      "Epoch 20/200 | Train 0.0777 | Val 0.0900\n",
      "Epoch 40/200 | Train 0.0376 | Val 0.0549\n",
      "Epoch 60/200 | Train 0.0133 | Val 0.0382\n",
      "Epoch 80/200 | Train 0.0069 | Val 0.0333\n",
      "Epoch 100/200 | Train 0.0042 | Val 0.0308\n",
      "Early stopping triggered at epoch 110\n",
      "\n",
      "---- Fold 4 ----\n",
      "Epoch 20/200 | Train 0.0603 | Val 0.0991\n",
      "Epoch 40/200 | Train 0.0181 | Val 0.0482\n",
      "Epoch 60/200 | Train 0.0144 | Val 0.0385\n",
      "Epoch 80/200 | Train 0.0050 | Val 0.0304\n",
      "Epoch 100/200 | Train 0.0023 | Val 0.0286\n",
      "Early stopping triggered at epoch 104\n",
      "\n",
      "---- Fold 5 ----\n",
      "Epoch 20/200 | Train 0.0777 | Val 0.1017\n",
      "Epoch 40/200 | Train 0.0341 | Val 0.0457\n",
      "Epoch 60/200 | Train 0.0169 | Val 0.0339\n",
      "Epoch 80/200 | Train 0.0135 | Val 0.0190\n",
      "Epoch 100/200 | Train 0.0092 | Val 0.0072\n",
      "Epoch 120/200 | Train 0.0054 | Val 0.0064\n",
      "Epoch 140/200 | Train 0.0035 | Val 0.0097\n",
      "Early stopping triggered at epoch 147\n",
      "\n",
      ">>> Mean F1 for E_64_32_16: 0.9980\n"
     ]
    }
   ],
   "source": [
    "# ========================================================\n",
    "# Stratified 5-Fold Training of Models A–E\n",
    "# ========================================================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "\n",
    "# Initialize the 5-fold splitter\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert full dataset to tensors for cross-validation\n",
    "X_full_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_full_tensor = torch.tensor(y.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Hidden layer configurations for each model\n",
    "model_configs = {\n",
    "    \"A_64_32\": [64, 32],\n",
    "    \"B_256\": [256],\n",
    "    \"C_64_32_16_8\": [64, 32, 16, 8],\n",
    "    \"D_128_32_128\": [128, 32, 128],\n",
    "    \"E_64_32_16\": [64, 32, 16]\n",
    "}\n",
    "\n",
    "# Container for all model results\n",
    "results = {}\n",
    "\n",
    "for model_name, hidden_sizes in model_configs.items():\n",
    "    \"\"\"\n",
    "    Perform stratified 5-fold training, validation, and metric collection\n",
    "    for a single MLP architecture. Tracks loss curves, ROC curves,\n",
    "    confusion matrices, and fold-wise performance metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Identifier for the model architecture being trained.\n",
    "\n",
    "    hidden_sizes : list of int\n",
    "        Sequence of hidden layer sizes defining the MLP architecture.\n",
    "\n",
    "    Updates\n",
    "    -------\n",
    "    results[model_name] : dict\n",
    "        Stores per-fold metrics, ROC data, confusion matrices, and\n",
    "        training/validation loss histories.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n\\n==============================\")\n",
    "    print(f\"Training Model {model_name}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    fold_metrics = []\n",
    "    fold_conf_mats = []\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    oof_probs = []\n",
    "    oof_labels = []\n",
    "\n",
    "    # ----- Begin 5-fold loop -----\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(X.values, y.values),\n",
    "        start=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train and evaluate the model for a single cross-validation fold.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_idx : array-like\n",
    "            Row indices for the training split.\n",
    "\n",
    "        val_idx : array-like\n",
    "            Row indices for the validation split.\n",
    "\n",
    "        Updates\n",
    "        -------\n",
    "        fold_train_losses : list\n",
    "            Stores per-epoch training losses for each fold.\n",
    "\n",
    "        fold_val_losses : list\n",
    "            Stores per-epoch validation losses for each fold.\n",
    "\n",
    "        fold_metrics : list\n",
    "            Stores optimal threshold metrics for each fold.\n",
    "\n",
    "        fold_conf_mats : list\n",
    "            Stores confusion matrix for each fold.\n",
    "\n",
    "        oof_probs : list\n",
    "            Sigmoid probabilities from out-of-fold predictions.\n",
    "\n",
    "        oof_labels : list\n",
    "            Ground-truth labels corresponding to out-of-fold predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\n---- Fold {fold} ----\")\n",
    "\n",
    "        X_train_f = X_full_tensor[train_idx]\n",
    "        y_train_f = y_full_tensor[train_idx]\n",
    "        X_val_f = X_full_tensor[val_idx]\n",
    "        y_val_f = y_full_tensor[val_idx]\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_f, y_train_f)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Initialize model\n",
    "        model = MLP(\n",
    "            input_size=24,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=1\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=1e-3,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience = 15\n",
    "        counter = 0\n",
    "\n",
    "        # ----- Epoch Loop -----\n",
    "        for epoch in range(200):\n",
    "            \"\"\"\n",
    "            Execute one epoch of forward/backward passes, gradient updates,\n",
    "            and validation evaluation.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            epoch : int\n",
    "                Training epoch number.\n",
    "\n",
    "            Updates\n",
    "            -------\n",
    "            train_losses : list of float\n",
    "                Adds average training loss per epoch.\n",
    "\n",
    "            val_losses : list of float\n",
    "                Adds validation loss per epoch.\n",
    "\n",
    "            best_val_loss : float\n",
    "                Tracks minimum validation loss across epochs.\n",
    "\n",
    "            counter : int\n",
    "                Counts epochs since last improvement for early stopping.\n",
    "            \"\"\"\n",
    "\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for xb, yb in train_loader:\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_logits = model(X_val_f)\n",
    "                val_loss = loss_fn(val_logits, y_val_f).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/200 | \"\n",
    "                    f\"Train {avg_train_loss:.4f} | Val {val_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        fold_train_losses.append(train_losses)\n",
    "        fold_val_losses.append(val_losses)\n",
    "\n",
    "        # ----- Probability Predictions for Metrics -----\n",
    "        with torch.no_grad():\n",
    "            val_probs = torch.sigmoid(model(X_val_f)).numpy().ravel()\n",
    "            y_val_np = y_val_f.numpy().ravel().astype(int)\n",
    "\n",
    "        # Optimal threshold selection\n",
    "        best_thr, best_m, best_J = threshold_sweep(y_val_np, val_probs)\n",
    "\n",
    "        # Confusion matrix\n",
    "        y_pred_best = (val_probs >= best_thr).astype(int)\n",
    "        cm = confusion_matrix(y_val_np, y_pred_best)\n",
    "        fold_conf_mats.append(cm)\n",
    "\n",
    "        oof_probs.append(val_probs)\n",
    "        oof_labels.append(y_val_np)\n",
    "\n",
    "        metrics_record = best_m.copy()\n",
    "        metrics_record[\"auc\"] = roc_auc_score(y_val_np, val_probs)\n",
    "        metrics_record[\"threshold\"] = best_thr\n",
    "        metrics_record[\"youden_J\"] = best_J\n",
    "        metrics_record[\"params\"] = count_trainable_params(model)\n",
    "\n",
    "        fold_metrics.append(metrics_record)\n",
    "\n",
    "    # ----- Aggregate ROC Across Folds -----\n",
    "    oof_probs = np.concatenate(oof_probs)\n",
    "    oof_labels = np.concatenate(oof_labels)\n",
    "    fpr, tpr, _ = roc_curve(oof_labels, oof_probs)\n",
    "    auc_val = roc_auc_score(oof_labels, oof_probs)\n",
    "\n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        \"fold_metrics\": fold_metrics,\n",
    "        \"train_losses\": fold_train_losses,\n",
    "        \"val_losses\": fold_val_losses,\n",
    "        \"conf_mats\": fold_conf_mats,\n",
    "        \"roc\": (fpr, tpr, auc_val),\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"\\n>>> Mean F1 for {model_name}: \"\n",
    "        f\"{np.mean([m['f1'] for m in fold_metrics]):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e71ce11-e0ce-4ed6-8e66-747a4bb17313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Utility Functions and Directory Management\n",
    "# =========================================================\n",
    "\"\"\"\n",
    "Utility functions for directory creation and figure saving used\n",
    "throughout CKD model evaluation. Ensures consistent output\n",
    "structure across all trained models and result artifacts.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_model_output_dir(model_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a results directory for storing model outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Name of the model architecture.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to the created model directory under Results/.\n",
    "    \"\"\"\n",
    "    base_dir = \"Results\"\n",
    "    model_dir = os.path.join(base_dir, f\"{model_name}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "def save_figure(path: str):\n",
    "    \"\"\"\n",
    "    Save the current matplotlib figure to the specified path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Full file path including filename for saving the figure.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Writes the figure to disk.\n",
    "    \"\"\"\n",
    "    plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde087e3-5f3e-4875-9920-e7652244cddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Loss curve -> Results/A_64_32/loss_curve.png\n",
      "[Saved] Loss curve -> Results/B_256/loss_curve.png\n",
      "[Saved] Loss curve -> Results/C_64_32_16_8/loss_curve.png\n",
      "[Saved] Loss curve -> Results/D_128_32_128/loss_curve.png\n",
      "[Saved] Loss curve -> Results/E_64_32_16/loss_curve.png\n",
      "[Saved] ROC curves -> Results/ROC_All_Models.png\n",
      "[Saved] Confusion matrix -> Results/A_64_32/confusion_matrix.png\n",
      "[Saved] Confusion matrix -> Results/B_256/confusion_matrix.png\n",
      "[Saved] Confusion matrix -> Results/C_64_32_16_8/confusion_matrix.png\n",
      "[Saved] Confusion matrix -> Results/D_128_32_128/confusion_matrix.png\n",
      "[Saved] Confusion matrix -> Results/E_64_32_16/confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall (Sensitivity)</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Youden J</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_64_32</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3777.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_256</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>6657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_64_32_16_8</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>4465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D_128_32_128</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9867</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9867</td>\n",
       "      <td>12001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_64_32_16</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>4321.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Accuracy  Precision  Recall (Sensitivity)  Specificity  \\\n",
       "0       A_64_32    1.0000     1.0000                   1.0       1.0000   \n",
       "1         B_256    0.9925     0.9884                   1.0       0.9800   \n",
       "2  C_64_32_16_8    0.9975     0.9961                   1.0       0.9933   \n",
       "3  D_128_32_128    0.9950     0.9922                   1.0       0.9867   \n",
       "4    E_64_32_16    0.9975     0.9961                   1.0       0.9933   \n",
       "\n",
       "   F1 Score     AUC  Youden J  Parameters  \n",
       "0    1.0000  1.0000    1.0000      3777.0  \n",
       "1    0.9941  0.9993    0.9800      6657.0  \n",
       "2    0.9980  0.9997    0.9933      4465.0  \n",
       "3    0.9960  0.9995    0.9867     12001.0  \n",
       "4    0.9980  0.9999    0.9933      4321.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] CSV -> Results/model_comparison_table.csv\n",
      "[Saved] LaTeX -> Results/model_comparison_table.tex\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Save Loss Curves, ROC Curves, Confusion Matrices, Tables\n",
    "# =========================================================\n",
    "\"\"\"\n",
    "Generate and save visualizations and summary tables for all CKD\n",
    "MLP architectures. Outputs include loss curves, ROC curves,\n",
    "confusion matrices, and a combined model comparison table.\n",
    "\"\"\"\n",
    "\n",
    "model_output_dirs = {}\n",
    "os.makedirs(\"Results\", exist_ok=True)\n",
    "\n",
    "# ----- Loss Curves -----\n",
    "for model_name, res in results.items():\n",
    "    \"\"\"\n",
    "    Generate and save the per-fold training and validation loss curves\n",
    "    for a given model architecture.\n",
    "\n",
    "    Updates\n",
    "    -------\n",
    "    Saves loss_curve.png to each model's Results directory.\n",
    "    \"\"\"\n",
    "    outdir = create_model_output_dir(model_name)\n",
    "    model_output_dirs[model_name] = outdir\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    for fold_idx, losses in enumerate(res[\"train_losses\"], start=1):\n",
    "        plt.plot(losses, label=f\"Train Fold {fold_idx}\", alpha=0.5)\n",
    "\n",
    "    for fold_idx, losses in enumerate(res[\"val_losses\"], start=1):\n",
    "        plt.plot(losses, label=f\"Val Fold {fold_idx}\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.title(f\"Loss vs Epochs — {model_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"BCE Loss\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    save_figure(os.path.join(outdir, \"loss_curve.png\"))\n",
    "    print(f\"[Saved] Loss curve -> {outdir}/loss_curve.png\")\n",
    "\n",
    "# ----- ROC Curves -----\n",
    "\"\"\"\n",
    "Aggregate ROC curves across all architectures and save the final\n",
    "combined ROC figure.\n",
    "\"\"\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for model_name, res in results.items():\n",
    "    fpr, tpr, auc_val = res[\"roc\"]\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC={auc_val:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves (Models A–E)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "save_figure(\"Results/ROC_All_Models.png\")\n",
    "print(\"[Saved] ROC curves -> Results/ROC_All_Models.png\")\n",
    "\n",
    "# ----- Confusion Matrices -----\n",
    "\"\"\"\n",
    "Generate and save aggregated confusion matrices for each model\n",
    "across all 5 folds.\n",
    "\"\"\"\n",
    "import itertools\n",
    "\n",
    "for model_name, res in results.items():\n",
    "    outdir = model_output_dirs[model_name]\n",
    "    cms = np.sum(res[\"conf_mats\"], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cms, cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix — {model_name}\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    ticks = [0, 1]\n",
    "    plt.xticks(ticks, [\"Pred 0\", \"Pred 1\"])\n",
    "    plt.yticks(ticks, [\"True 0\", \"True 1\"])\n",
    "\n",
    "    threshold = cms.max() / 2\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(\n",
    "            j, i, cms[i, j],\n",
    "            ha=\"center\",\n",
    "            color=\"white\" if cms[i, j] > threshold else \"black\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_figure(os.path.join(outdir, \"confusion_matrix.png\"))\n",
    "    print(f\"[Saved] Confusion matrix -> {outdir}/confusion_matrix.png\")\n",
    "\n",
    "# ----- Model Comparison Table -----\n",
    "rows = []\n",
    "for model_name, res in results.items():\n",
    "    fold_m = res[\"fold_metrics\"]\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": np.mean([m[\"accuracy\"] for m in fold_m]),\n",
    "        \"Precision\": np.mean([m[\"precision\"] for m in fold_m]),\n",
    "        \"Recall (Sensitivity)\": np.mean([m[\"recall\"] for m in fold_m]),\n",
    "        \"Specificity\": np.mean([m[\"specificity\"] for m in fold_m]),\n",
    "        \"F1 Score\": np.mean([m[\"f1\"] for m in fold_m]),\n",
    "        \"AUC\": np.mean([m[\"auc\"] for m in fold_m]),\n",
    "        \"Youden J\": np.mean([m[\"youden_J\"] for m in fold_m]),\n",
    "        \"Parameters\": np.mean([m[\"params\"] for m in fold_m]),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "num_cols = results_df.select_dtypes(include=[float, int]).columns\n",
    "results_df[num_cols] = results_df[num_cols].apply(lambda x: x.round(4))\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "csv_path = \"Results/model_comparison_table.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"[Saved] CSV -> {csv_path}\")\n",
    "\n",
    "latex_path = \"Results/model_comparison_table.tex\"\n",
    "latex_table = results_df.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    column_format=\"lcccccccc\",\n",
    "    caption=\"Performance comparison of MLP architectures (Models A–E).\",\n",
    "    label=\"tab:mlp_comparison\"\n",
    ")\n",
    "with open(latex_path, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"[Saved] LaTeX -> {latex_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95688bb0-350f-45b2-a116-815274d15a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Running SHAP Explainability\n",
      "==========================\n",
      "\n",
      "Best Model by F1: A_64_32\n",
      "Retraining best model on full dataset...\n",
      "Finished training best model.\n",
      "SHAP array shape: (200, 24)\n",
      "[Saved] SHAP summary → Results/A_64_32/SHAP_summary.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/wtlx_zr9185f41dv1h0ynwp80000gn/T/ipykernel_61254/124124582.py:85: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] dependence plot for appet\n",
      "[Saved] dependence plot for rbc\n",
      "[Saved] dependence plot for sg\n",
      "[Saved] dependence plot for dm\n",
      "[Saved] dependence plot for pc\n",
      "\n",
      "SHAP explainability complete.\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# SHAP Explainability for Best Model\n",
    "# ===================================\n",
    "\"\"\"\n",
    "Compute SHAP values for the best-performing MLP architecture.\n",
    "Includes training the best model on the full dataset, computing\n",
    "SHAP values using the unified PyTorch Explainer, and generating\n",
    "summary and dependence plots for the most influential features.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(\"Running SHAP Explainability\")\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "# ----- Identify Best Model -----\n",
    "best_model_name = max(\n",
    "    results.keys(),\n",
    "    key=lambda m: np.mean([fm[\"f1\"] for fm in results[m][\"fold_metrics\"]])\n",
    ")\n",
    "print(f\"Best Model by F1: {best_model_name}\")\n",
    "\n",
    "# ----- Retrain Best Model on Full Dataset -----\n",
    "print(\"Retraining best model on full dataset...\")\n",
    "\n",
    "best_hidden = model_configs[best_model_name]\n",
    "best_model = MLP(input_size=24, hidden_sizes=best_hidden, output_size=1)\n",
    "best_model.eval()\n",
    "\n",
    "optimizer = torch.optim.AdamW(best_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "dataset_full = TensorDataset(X_full_tensor, y_full_tensor)\n",
    "loader_full = DataLoader(dataset_full, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(50):\n",
    "    best_model.train()\n",
    "    for xb, yb in loader_full:\n",
    "        logits = best_model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "best_model.eval()\n",
    "print(\"Finished training best model.\")\n",
    "\n",
    "# ----- SHAP Input Preparation -----\n",
    "background = X.values[:100].astype(np.float32)\n",
    "shap_input = X.values[:200].astype(np.float32)\n",
    "\n",
    "def predict_numpy(X_np):\n",
    "    \"\"\"\n",
    "    Wrapper function for SHAP to perform model inference using numpy arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_np : ndarray of float32, shape (n_samples, n_features)\n",
    "        Input data in numpy format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray of float\n",
    "        Predicted sigmoid probabilities.\n",
    "    \"\"\"\n",
    "    X_t = torch.tensor(X_np, dtype=torch.float32)\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = torch.sigmoid(best_model(X_t)).numpy().ravel()\n",
    "    return out\n",
    "\n",
    "# ----- Compute SHAP Values -----\n",
    "masker = shap.maskers.Independent(background)\n",
    "explainer = shap.Explainer(predict_numpy, masker)\n",
    "\n",
    "shap_values = explainer(shap_input)\n",
    "shap_matrix = shap_values.values\n",
    "\n",
    "print(\"SHAP array shape:\", shap_matrix.shape)\n",
    "\n",
    "# ----- Output Directory -----\n",
    "best_outdir = model_output_dirs[best_model_name]\n",
    "\n",
    "# ----- Summary Plot -----\n",
    "shap.summary_plot(\n",
    "    shap_matrix,\n",
    "    shap_input,\n",
    "    feature_names=X.columns,\n",
    "    max_display=10,\n",
    "    show=False\n",
    ")\n",
    "save_figure(os.path.join(best_outdir, \"SHAP_summary.png\"))\n",
    "print(f\"[Saved] SHAP summary → {best_outdir}/SHAP_summary.png\")\n",
    "\n",
    "# ----- Dependence Plots -----\n",
    "top_features = np.argsort(np.abs(shap_matrix).mean(axis=0))[-5:]\n",
    "top_feature_names = X.columns[top_features]\n",
    "\n",
    "for feat in top_feature_names:\n",
    "    shap.dependence_plot(\n",
    "        feat,\n",
    "        shap_matrix,\n",
    "        shap_input,\n",
    "        feature_names=X.columns,\n",
    "        show=False\n",
    "    )\n",
    "    save_figure(os.path.join(best_outdir, f\"dependence_{feat}.png\"))\n",
    "    print(f\"[Saved] dependence plot for {feat}\")\n",
    "\n",
    "print(\"\\nSHAP explainability complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d08bf-055a-4674-9d66-790d3a0c2382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
